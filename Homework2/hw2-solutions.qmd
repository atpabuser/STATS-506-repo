---
title: "HW2 Solutions"
format: pdf
editor: visual
---

### Github link: https://github.com/atpabuser/STATS-506-repo/tree/main/Homework2

# Problem 1

Consider a 1-dimensional random walk with the following rules:

1.  Start at 0.
2.  At each step, move +1 or -1 with 50/50 probability. If +1 is chosen, 5% of the time move +10 instead. If -1 is chosen, 20% of the time move -3 instead. Repeat steps 2-4 $n$ times. (Note that if the +10 is chosen, it’s not +1 then +10, it is just +10.)

Write a function to determine the end position of this random walk.

The input and output should be:

Input: The number of steps Output: The final position of the walk

### Version 1: Using a loop

**Marginal computations:**

-   (+1): $0.5 \times 0.95 = 0.475$
-   (+10): $0.5 \times 0.05 = 0.025$
-   (–1): $0.5 \times 0.80 = 0.40$
-   (–3): $0.5 \times 0.20 = 0.10$

```{r}
#' Random Walk Simulation
#'
#' Simulate a 1d random walk. At each step, the walker moves +1 or -1 with
#' 50-50 odds.+1 becomes +10 with 5% chance, and -1 becomes -3 with 20% odds
#'
#' @param n The number of steps
#' @return the final position of the walk

random_walk1 <- function(n) {
  #Start at position 0
  position <- 0
  #steps vector
  steps <- c(1,10,-1,-3)
  probabilities <-c(0.475, 0.025, 0.400, 0.100)
  
  for (i in 1:n){
  #do single random draw to get move for the step 
    move <-sample(x=steps, size=1, prob=probabilities)
  
  #update position
    position <- position + move
  }
  return(position)
}
```

### Version 2: No loop, vectorized

To answer the question: The order of steps does not matter, we only need the final position since the final position, (the sum), is the sum of i.i.d steps

```{r}
#' Vectorized 1D Random Walk Simulation 
#' 
#' @param n The number of steps
#' @return Sum: Final position 
#' 

random_walk2 <- function(n) {
  #steps vector
  steps <- c(1,10,-1,-3)
  probabilities <-c(0.475, 0.025, 0.400, 0.100)
  
  #sample all at once and sum to get final position
  
  return(sum(sample(steps, size = n, replace = TRUE, prob = probabilities)))
}

```

### Version 3: Using `apply()` function

```{r}
#' using sapply() 1D Random Walk Simulation 
#' 
#' @param n The number of steps
#' @return Sum: Final position 
#' 

random_walk3 <- function(n) {
  steps <- c(1,10,-1,-3)
  probabilities <-c(0.475, 0.025, 0.400, 0.100)
  all_steps <- sapply(1:n, function(i) 
    {sample(x=steps, size=1,replace=TRUE, prob=probabilities)})
  
  return(sum(all_steps))
} 

```

### Demonstrate that all versions work by running the following:

```{r}
random_walk1(10)
random_walk2(10)
random_walk3(10)
random_walk1(1000)
random_walk2(1000)
random_walk3(1000)

```

### 1b. Demonstrate that the three versions can give the same result. Show this for both $n=10$ and $n=1000$

To control for randomness, we fix the seed before calling each random_walk function. I will do a different seed for $n=10$ and $n=1000$

### Case 1: $n=10$

```{r}
set.seed(123)
val<-10
cat("The final value for n=",val,"using random_walk1() is:",random_walk1(val))

set.seed(123)
cat("\nThe final value for n=",val,"using random_walk2() is:",random_walk2(val))

set.seed(123)
cat("\nThe final value for n=",val,"using random_walk3() is:",random_walk3(val))


```

### Case 2: $n=1000$

```{r}
set.seed(555)
val2<-1000
cat("The final value for n=",val2,"using random_walk1() is:",random_walk1(val2))

set.seed(555)
cat("\nThe final value for n=",val2,"using random_walk2() is:",random_walk2(val2))

set.seed(555)
cat("\nThe final value for n=",val2,"using random_walk3() is:",random_walk3(val2))

```

### 1c.

Use the microbenchmark package to clearly demonstrate the speed of the implementations.Compare performance with a low input and a large input. Discuss results.

```{r}
library(microbenchmark)
n_val1<-1000
results_nval1 <-microbenchmark(
  loop <- random_walk1(n_val1),
  vectorized <- random_walk2(n_val1),
  apply <- random_walk3(n_val1)
)

print(results_nval1)


```

```{r}

n_val2<-100000
results_nval2 <-microbenchmark(
  loop <- random_walk1(n_val2),
  vectorized <- random_walk2(n_val2),
  apply <- random_walk3(n_val2)
)

print(results_nval2)


```

So, I expected the vectorized function to clearly be the fastest, (which it is in both cases) but I am quite surprised that my for-loop implementation is faster than the implementation using `sapply()`. I did some googling and found out the following links:

1.  https://www.reddit.com/r/rstats/comments/9s778r/in_benchmark_tests_a_forloop_was_almost_50_faster/
2.  https://www.tylermw.com/posts/data_analysis/theres-no-need-to-apply-yourself.html

In either case, however, the best thing to do for the quickest implementation is vectorization.

### 1d.

Define a helper function to compute the probability the random walk ends at 0. Use vectorized random_walk implementation. WLOG it does not matter which one I choose as long as I fix the seed.

```{r}

#' @param n number of steps in the random walk
#' 
#' @param num_sims number of simulations we run
#' 
#' @return estimated probability random walk ends at 0

prob_zero_rw <- function(n,num_sims){
  #run the random walk for the specified simulation size
  final_positions <- numeric(num_sims)
  
  #loop to get final positions
  for (i in 1:num_sims){
    final_positions[i] <- random_walk2(n)
  }
  successes <- sum(final_positions == 0)
  return (successes/num_sims)
}
```

### Case 1: $n$=10

```{r}
set.seed(929)
num_sims <- 10000
prob_10 <- prob_zero_rw(10,num_sims)
cat("The probability of the random walk going to zero after 10 steps is:",prob_10)

```

### Case 2: $n$=100

```{r}
set.seed(929)
num_sims <- 10000
prob_100 <- prob_zero_rw(100,num_sims)
cat("The probability of the random walk going to zero after 100 steps is:",prob_100)

```

### Case 3: $n$=1000

```{r}
set.seed(929)
num_sims <- 10000
prob_1000 <- prob_zero_rw(1000,num_sims)
cat("The probability of the random walk going to zero after 1000 steps is:",prob_1000)

```

As we increase the number of steps, the probability that the random walk ends at 0 converges to 0. As I have shown above, the probability gets closer and closer to 0 as I go from n=10,100,1000. This makes sense because as you increase the number of steps of the random walk, the range of possible positions it ends at grows much larger, so the probability we land at 0 decreases.

Here is a graph visualizing this:

```{r}
set.seed(929)

#define the range of steps we plot
n_values <-seq(10,1000,10)

#compute probabilities
probs <- sapply(n_values, prob_zero_rw,num_sims=5000)
plot(x=n_values, y=probs, col="blue", xlab="Number of Steps", ylab="Probabilities")

```

Can see it converging to 0 as $n$ increases.

# Problem 2

Using a Monte Carlo simulation, estimate the average number of cars that pass an intersection per day under the following assumptions:

-   From midnight until 7 AM, the distribution of cars per hour is Poisson with mean 1.
-   From 9am to 4pm, the distribution of cars per hour is Poisson with mean 8.
-   From 6pm to 11pm, the distribution of cars per hour is Poisson with mean 12.
-   During rush hours (8am and 5pm), the distribution of cars per hour is Normal with mean 60 and variance 12

**Do not use any loops**

For a poisson rv, the expected value is $n$$\lambda$. The theoretical expected value would be: (8 $\times$ 1)+(1 $\times$ 60)+ (8 $\times$ 8)+(1 $\times$ 60)+(6 $\times$ 12)=264

```{r}
n_simuls <- 10000
set.seed(123)

midnight_7am <- matrix(rpois(n_simuls*8, lambda=1), nrow=n_simuls, ncol=8)
rush_8am <- rnorm(n_simuls, mean = 60, sd = sqrt(12))
nine_4pm <- matrix(rpois(n_simuls * 8, lambda = 8), nrow = n_simuls, ncol = 8)
rush_5pm <- rnorm(n_simuls, mean = 60, sd = sqrt(12))
six_11pm <- matrix(rpois(n_simuls*6, lambda=12), nrow=n_simuls,ncol=6)

daily_sum <- rowSums(midnight_7am)+rush_8am+rowSums(nine_4pm)+rush_5pm+rowSums(six_11pm)

empirical_ev <-mean(daily_sum)
print(empirical_ev)

```

So, the empirical EV calculated by Monte Carlo is a close approximation to the theoretical EV. 

# Problem 3

### 3a 
Often in data analysis, we need to de-identify it. This is more important for studies of people, but let’s carry it out here. Remove any column that might uniquely identify a commercial. This includes but isn’t limited to things like brand, any URLs, the YouTube channel, or when it was published.

Report the dimensions of the data after removing these columns.

```{r}

youtube <- read.csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-03-02/youtube.csv')

#head(youtube)
#names(youtube)

identifying_cols <- c("brand", "superbowl_ads_dot_com_url", "youtube_url","title", "description", "thumbnail", "channel_title","published_at")

youtube_cleaned <- youtube[,!(names(youtube) %in% identifying_cols)]

print(dim(youtube_cleaned))

sum(is.na(youtube_cleaned$view_count))

```
247 rows and 17 columns. 

### 3b 

For each of the following variables, examine their distribution. Determine whether i) The variable could be used as is as the outcome in a linear regression model, ii) The variable can use a transformation prior to being used as the outcome in a linear regression model, or iii) The variable would not be appropriate to use as the outcome in a linear regression model.

For each variable, report which category it falls in. If it requires a transformation, carry such a transformation out and use that transformation going forward.

View counts
Like counts
Dislike counts
Favorite counts
Comment counts
(Hint: At least the majority of these variables are appropriate to use.)

### Examine the data

```{r}
hist(youtube_cleaned$view_count, main="Views", col="red")
hist(youtube_cleaned$like_count, main="Likes", col="red")
hist(youtube_cleaned$dislike_count, main="Dislikes", col="red")
hist(youtube_cleaned$favorite_count, main="Favorites", col="red")
hist(youtube_cleaned$comment_count, main="Comments", col="red")
```
1. View-count is right-skewed. Most ads have very little views, only some ads go viral. Consider a log transform. Category II
2. Likes: also right-skewed. Consider a log transform. Category II
3. dislike count: right-skewed, do a log transform. Category II
4. favorites: has no predictive value. Category III. In fact, it should be dropped since most of the videos have zero favorites and missing values. Also, youtube no longer records public favorite counts. 
5. Comments: also right-skewed, do a log transform. Category II

None of these variables are good match for Category I either. They are strongly skewed and have too many zeroes. 

###do log transforms and drop favorite_count
https://scicomp.stackexchange.com/questions/20629/when-should-log1p-and-expm1-be-used

I will use `log1p()` to handle zeroes present in the dataset. 

```{r}
#log transforms
youtube_cleaned$view_count_log <- log1p(youtube_cleaned$view_count)
youtube_cleaned$like_count_log <- log1p(youtube_cleaned$like_count)
youtube_cleaned$dislike_count_log <- log1p(youtube_cleaned$dislike_count)
youtube_cleaned$comment_count_log <- log1p(youtube_cleaned$comment_count)
#drop favorite_count
youtube_cleaned$favorite_count <- NULL
```


```{r}
summary(youtube_cleaned$view_count_log)
```

### 3c. 
For each variable in part b. that are appropriate, fit a linear regression model predicting them based upon each of the seven binary flags for characteristics of the ads, such as whether it is funny. Control for year as a continuous covariate. Discuss the results. Identify the direction of any statistically significant results.


### Answer

We are going to do 4 separate linear regressions using `lm()`. For each model, the predicted variable will be one of the log-transformed count variables we handled in 3b. The covariates/predictors will be the seven binary flags and we control for year as a continuous covariate. I will then use the `summary()` for each regression to look for predictors where the p-value < 0.05. 


### predicting log(view_count)
```{r}
model_views <- lm(view_count_log ~ funny + show_product_quickly + patriotic + 
                              celebrity + danger + animals + use_sex + year, 
                  data = youtube_cleaned)
summary(model_views)
```

```{r}
# Model 2: Predicting Log Like Counts
model_likes <- lm(like_count_log ~ funny + show_product_quickly + patriotic + 
                              celebrity + danger + animals + use_sex + year, 
                  data = youtube_cleaned)
summary(model_likes)
```

```{r}
# Model 3: Predicting Log DisLike Counts
model_dislikes <- lm(dislike_count_log ~ funny + show_product_quickly + patriotic + 
                                  celebrity + danger + animals + use_sex + year, 
                     data = youtube_cleaned)
summary(model_dislikes)
```

```{r}
# Model 4: Predicting Comment Counts
model_comments <- lm(comment_count_log ~ funny + show_product_quickly + patriotic + 
                                  celebrity + danger + animals + use_sex + year, 
                     data = youtube_cleaned)
summary(model_comments)
```
There are multiple things to address here: Let's start with the F-test. 
For View Count, F-statistic p-value: 0.631 means the model as a whole is not statistically significant i.e. the model with the seven flags and year together DOES NOT explain view counts better than a model only having an intercept. 

For like-count, F-statistic p-value of 0.0342 is significant but only the Year predictor was significant so year drives the significance

For Dislike-count, it is the same case as like-count, driven by Year. 

For Comment Count, the F statistic p-value is 0.06748, so the model is not significant but neither are any the predictors. 

Overall for the predictors, only YEAR was significant with a positive effect on likes and dislikes, which makes sense since newer advertisements get more attention and hence more likes and dislikes. 

For Comment Count, the coefficient for YEAR was positive but just shy of <0.05. 

For View Count, there was no significant predictors. 

Advertisement characteristics such as: funny, patriotic, celebrity, danger, animals, sex, showing product quickly were not statistically significant. This seems counterintuitive to me, but I speculate it may be for the following reasons: 
  i. Looking at an advertisement as a set of binary traits does not reflect real-world properties. Advertisements can satisfy multiple categories at once, i.e. multicollinearity
  ii. The variables we omitted in the beginning could have effected the statistical strength of our binary predictors. 
  iii. Sample size is not large enough. 


### 3d.
Consider only the outcome of view counts. Calculate $\hat{\beta}$ manually (without using `lm()`) by first creating a proper design matrix, then using matrix algebra to estimate $\hat{\beta}$. Confirm that you get the same result as `lm()` did in part c.


```{r}
model_vars <- c("view_count_log", "funny", "show_product_quickly", "patriotic", 
                "celebrity", "danger", "animals", "use_sex", "year")

youtube_model_data <- youtube_cleaned[, model_vars]

youtube_total <- na.omit(youtube_model_data) #the lm() function omits NA values, so we replicate this here. 
 
#Define X and Y

Y <- youtube_total$view_count_log
X <- model.matrix(~ funny + show_product_quickly + patriotic + celebrity + 
                  danger + animals + use_sex + year, youtube_total)

#Solve for beta hat manually

beta_hat <- solve(t(X) %*% X) %*% t(X) %*% Y

#convert beta_hat into vector
beta_hat_vector <- drop(beta_hat)

#Now compute the beta using lm()

model_view_count<- lm(view_count_log ~ funny + show_product_quickly + patriotic + 
               celebrity + danger + animals + use_sex + year, youtube_total)

#Comparison table 
comparison_table <- cbind(Beta_hat_manual = beta_hat, Beta_hat_lm = coef(model_view_count))
print(comparison_table)


#Check for equality
equal_check <- all.equal(beta_hat_vector, coef(model_view_count))
cat("The beta hat calculated by manual method matches lm beta hat?: ",equal_check)

```
So, they match. 






